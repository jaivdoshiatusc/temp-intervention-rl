name: a2c  # algorithm name

a2c:
  total_timesteps: 1000000          # Total training timesteps
  log_freq: 10000                   # Log every log_freq timesteps
  eval_freq: 10000                  # Evaluate every eval_freq timesteps
  save_freq: 100000                 # Save model every save_freq timesteps

  oversight_timesteps: 100000       # Number of timesteps to train with oversight
  blocker_train_freq: 10            # Train blocker every blocker_train_freq timesteps
  
  policy: CnnPolicy                 # Policy model to use (MlpPolicy, CnnPolicy, ...)
  policy_kwargs: {}                 # Additional arguments for policy model
  
  learning_rate: 1e-4               # Learning rate for A2C
  n_steps: 5                        # Number of steps in rollout
  gamma: 0.99                       # Discount factor
  gae_lambda: 0.95                  # GAE (Generalized Advantage Estimation) lambda
  ent_coef: 0.01                    # Entropy coefficient
  vf_coef: 0.5                      # Value function coefficient
  max_grad_norm: 0.5                # Max norm for gradient clipping
  rms_prop_eps: 1e-5                # Epsilon for RMSProp optimizer
  use_rms_prop: true                # Use RMSProp optimizer
  use_sde: false                    # Use State Dependent Exploration (SDE)
  sde_sample_freq: -1               # SDE sampling frequency
  normalize_advantage: false        # Normalize advantage estimates
  verbose: 1                        # Verbosity level